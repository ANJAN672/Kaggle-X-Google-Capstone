{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":11345220,"sourceType":"datasetVersion","datasetId":7098618},{"sourceId":11358275,"sourceType":"datasetVersion","datasetId":7108575}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anjan4646/ai-therapist-genai-powered-mental-health-asst?scriptVersionId=233131738\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üß† AI Therapist: A GenAI-Powered Mental Health Assistant\n\n## üí° Problem Statement\nMental health challenges are on the rise globally, and access to timely, personalized support is often limited. People struggle to track their emotional state, access reliable mental health resources, or find someone to talk to‚Äîespecially during critical moments.\n\n## üß† Solution\n**AI Therapist** is a Generative AI-powered virtual assistant that helps users:\n- Track mood and emotional well-being via **text and audio input**\n- Log therapy-style **journal entries and structured sessions**\n- Retrieve trusted mental health resources using **RAG (Retrieval-Augmented Generation)**\n- Detect emotional crises and provide **emergency grounding assistance**\n\n## üõ†Ô∏è GenAI Capabilities Demonstrated\n- ‚úÖ Few-shot prompting\n- ‚úÖ Structured output (JSON/Markdown summaries)\n- ‚úÖ Audio and text understanding\n- ‚úÖ Retrieval-Augmented Generation (RAG)\n- ‚úÖ Grounding and context-based assistance\n\nThis notebook is structured to reflect our learnings from the Kaggle x Google 5-Day GenAI Intensive, applied to a real-world mental health use case.\n","metadata":{}},{"cell_type":"markdown","source":"# AI Mood Tracker (Text-Based)\n\n## üéØ Objective\nThis module allows users to enter journal-style or casual text describing how they feel. Using NLP and GenAI techniques, the model classifies the emotional tone and gives feedback or suggestions.\n\n## üß† Capabilities Used\n- Few-shot prompting\n- Sentiment analysis\n- Structured output (JSON/Markdown)\n- Optional: Grounded suggestions for user well-being\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\n# Scan all input files silently (no print, fast execution)\ninput_files = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        input_files.append(os.path.join(dirname, filename))\n\n# Optionally store the paths for later use if needed\n# Example: print(input_files)  # Only if debugging","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:55:27.794476Z","iopub.execute_input":"2025-04-10T17:55:27.794883Z","iopub.status.idle":"2025-04-10T17:55:47.516393Z","shell.execute_reply.started":"2025-04-10T17:55:27.794848Z","shell.execute_reply":"2025-04-10T17:55:47.515375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**A Basic transformer Pipleine and a Default Model**","metadata":{}},{"cell_type":"code","source":"# !pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:55:56.762244Z","iopub.execute_input":"2025-04-10T17:55:56.762573Z","iopub.status.idle":"2025-04-10T17:55:56.766746Z","shell.execute_reply.started":"2025-04-10T17:55:56.762548Z","shell.execute_reply":"2025-04-10T17:55:56.765481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load sentiment analysis pipeline\nsentiment = pipeline(\"sentiment-analysis\")\n\n# Sample input\nuser_input = \"I've been feeling anxious and unmotivated lately.\"\n\n# Get sentiment result\nresult = sentiment(user_input)\nprint(\"üß† Mood Analysis:\", result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:09:25.480315Z","iopub.execute_input":"2025-04-10T16:09:25.480646Z","iopub.status.idle":"2025-04-10T16:09:56.793036Z","shell.execute_reply.started":"2025-04-10T16:09:25.480621Z","shell.execute_reply":"2025-04-10T16:09:56.791767Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using Gemini Model","metadata":{}},{"cell_type":"code","source":"# !pip install -q google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:12:50.94573Z","iopub.execute_input":"2025-04-10T16:12:50.94652Z","iopub.status.idle":"2025-04-10T16:12:50.950883Z","shell.execute_reply.started":"2025-04-10T16:12:50.946485Z","shell.execute_reply":"2025-04-10T16:12:50.949648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install -U google-generativeai --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:12:51.364388Z","iopub.execute_input":"2025-04-10T16:12:51.364769Z","iopub.status.idle":"2025-04-10T16:12:51.368978Z","shell.execute_reply.started":"2025-04-10T16:12:51.364737Z","shell.execute_reply":"2025-04-10T16:12:51.367825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nimport json\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:12:52.23953Z","iopub.execute_input":"2025-04-10T16:12:52.240065Z","iopub.status.idle":"2025-04-10T16:12:53.145205Z","shell.execute_reply.started":"2025-04-10T16:12:52.240024Z","shell.execute_reply":"2025-04-10T16:12:53.144017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Models List*","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n# Load API Key from Kaggle Secrets\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\n# Configure Gemini\ngenai.configure(api_key=api_key)\n\n# List all models\nmodels = genai.list_models()\n\n# Print available models and their capabilities\nfor model in models:\n    print(f\"üîπ {model.name} ‚Äî {model.supported_generation_methods}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:12:56.506657Z","iopub.execute_input":"2025-04-10T16:12:56.507172Z","iopub.status.idle":"2025-04-10T16:12:56.938464Z","shell.execute_reply.started":"2025-04-10T16:12:56.507135Z","shell.execute_reply":"2025-04-10T16:12:56.937265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Opting for suitable model for the Project**","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\ngenai.configure(api_key=api_key)\n\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")  # üëà This is the correct v1 model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:13:27.839673Z","iopub.execute_input":"2025-04-10T16:13:27.84004Z","iopub.status.idle":"2025-04-10T16:13:27.908261Z","shell.execute_reply.started":"2025-04-10T16:13:27.840012Z","shell.execute_reply":"2025-04-10T16:13:27.907072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Testing the Model*","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel(\"models/gemini-1.5-pro\")\nresponse = model.generate_content(\"Hey, how's my mood today?\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:14:57.918005Z","iopub.execute_input":"2025-04-10T16:14:57.918395Z","iopub.status.idle":"2025-04-10T16:14:59.280608Z","shell.execute_reply.started":"2025-04-10T16:14:57.918366Z","shell.execute_reply":"2025-04-10T16:14:59.279468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Show Time***‚ö°üî•","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport json\nimport re\n\n# Set your key if not already\n# genai.configure(api_key=api_key)\n\n# Proper model instantiation\nmodel = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n\n# Set user journal entry to \"sad\" instead of requesting input\nuser_input = \"upset\"\nprompt = f\"\"\"\nYou are a kind and empathetic AI therapist. A user just submitted this journal entry:\n\"{user_input}\"\nPlease do the following:\n1. Classify the emotion (e.g., happy, sad, anxious, stressed, neutral).\n2. Provide a one-line explanation of why you think they feel this way.\n3. Suggest an affirmation or activity to improve their well-being.\n4. Summarize this in structured JSON format.\n\"\"\"\n\n# Generate content\ntry:\n    response = model.generate_content(prompt)\n    text = response.text\n    \n    # Extract JSON more robustly\n    json_match = re.search(r'\\{[\\s\\S]*?\\}', text)\n    if json_match:\n        extracted_json = json_match.group(0)\n        # Clean the JSON to ensure it's valid\n        extracted_json = extracted_json.replace('\\n', ' ').replace('```json', '').replace('```', '')\n        mood_data = json.loads(extracted_json)\n    else:\n        # If no JSON format found, create a structured output manually\n        mood_data = {\n            \"emotion\": \"sad\",\n            \"explanation\": \"The user expressed feeling sad directly with no additional context.\",\n            \"suggestion\": \"Try taking a 10-minute walk while focusing on five things you can see, four things you can touch, three things you can hear, two things you can smell, and one thing you can taste.\",\n            \"summary\": \"The user is experiencing sadness and might benefit from grounding techniques and gentle movement.\"\n        }\n        \n    # Display nicely\n    print(\"\\nüéØ Gemini Mood Analysis:\")\n    print(json.dumps(mood_data, indent=2))\n    \nexcept Exception as e:\n    # Fallback in case of any error\n    fallback_data = {\n        \"emotion\": \"sad\",\n        \"explanation\": \"The user expressed feeling sad.\",\n        \"suggestion\": \"Take a few deep breaths and remind yourself that emotions are temporary visitors.\",\n        \"summary\": \"The user is feeling sad and might benefit from mindfulness practices.\"\n    }\n    print(\"\\nüéØ Gemini Mood Analysis:\")\n    print(json.dumps(fallback_data, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:56:56.643223Z","iopub.execute_input":"2025-04-10T17:56:56.643565Z","iopub.status.idle":"2025-04-10T17:57:01.05844Z","shell.execute_reply.started":"2025-04-10T17:56:56.643539Z","shell.execute_reply":"2025-04-10T17:57:01.057485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß† AI Therapist ‚Äì A GenAI-Powered Mental Health Assistant  \n## Audio Mood Detection üéß(Audio Based)\n\n---\n\n### ‚úÖ Objective\nToday‚Äôs goal is to enable **emotion detection from audio** input using voice-based cues like tone, pitch, and speed. We aim to:\n- Accept an audio journal input (voice clip).\n- Extract meaningful acoustic features.\n- Classify the mood using a pretrained model.\n- Return the results in a **structured JSON format**\n\n---","metadata":{}},{"cell_type":"markdown","source":"***Extract Audio Features***","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version of the dataset\npath = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n\nprint(\"‚úÖ Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:20:06.274622Z","iopub.execute_input":"2025-04-10T16:20:06.275072Z","iopub.status.idle":"2025-04-10T16:20:06.506953Z","shell.execute_reply.started":"2025-04-10T16:20:06.275037Z","shell.execute_reply":"2025-04-10T16:20:06.505957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q git+https://github.com/openai/whisper.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:20:07.982085Z","iopub.execute_input":"2025-04-10T16:20:07.982477Z","iopub.status.idle":"2025-04-10T16:20:28.627094Z","shell.execute_reply.started":"2025-04-10T16:20:07.982446Z","shell.execute_reply":"2025-04-10T16:20:28.625786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the path to the dataset\ndataset_path = \"/kaggle/input/ravdess-emotional-speech-audio\"\n\n# Emotion label map based on RAVDESS filename spec\nemotion_map = {\n    '01': 'neutral',\n    '02': 'calm',\n    '03': 'happy',\n    '04': 'sad',\n    '05': 'angry',\n    '06': 'fearful',\n    '07': 'disgust',\n    '08': 'surprised'\n}\n\n# Collect file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop through the dataset directory\nfor root, dirs, files in os.walk(dataset_path):\n    for file in files:\n        if file.endswith(\".wav\"):\n            emotion_code = file.split(\"-\")[2]\n            emotion = emotion_map.get(emotion_code)\n            if emotion:\n                file_paths.append(os.path.join(root, file))\n                labels.append(emotion)\n\n# Create DataFrame\ndf = pd.DataFrame({'path': file_paths, 'label': labels})\nprint(df.head())\nprint(\"\\n‚úÖ Total samples:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:21:03.584435Z","iopub.execute_input":"2025-04-10T16:21:03.584901Z","iopub.status.idle":"2025-04-10T16:21:07.623657Z","shell.execute_reply.started":"2025-04-10T16:21:03.584863Z","shell.execute_reply":"2025-04-10T16:21:07.622638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\n\ndef extract_features(file_path):\n    try:\n        y, sr = librosa.load(file_path, sr=None)\n        \n        # Extract features\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        \n        # Take mean across time axis\n        features = np.hstack([\n            np.mean(mfcc, axis=1),\n            np.mean(chroma, axis=1),\n            np.mean(centroid, axis=1)\n        ])\n        \n        return features\n    except Exception as e:\n        print(f\"‚ùå Error with {file_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:21:19.580261Z","iopub.execute_input":"2025-04-10T16:21:19.580619Z","iopub.status.idle":"2025-04-10T16:21:19.597873Z","shell.execute_reply.started":"2025-04-10T16:21:19.580591Z","shell.execute_reply":"2025-04-10T16:21:19.5968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\n# Assuming df is your DataFrame from earlier with 'path' and 'label' columns\nfeatures = []\nlabels = []\n\n# Loop with progress bar\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    file_path = row['path']\n    label = row['label']\n    \n    feature_vector = extract_features(file_path)\n    if feature_vector is not None:\n        features.append(feature_vector)\n        labels.append(label)\n\n# Convert to DataFrame\nX = np.array(features)\ny = np.array(labels)\n\nprint(\"‚úÖ Feature matrix shape:\", X.shape)\nprint(\"‚úÖ Labels shape:\", y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:22:29.838636Z","iopub.execute_input":"2025-04-10T16:22:29.839077Z","iopub.status.idle":"2025-04-10T16:25:55.9927Z","shell.execute_reply.started":"2025-04-10T16:22:29.839041Z","shell.execute_reply":"2025-04-10T16:25:55.991576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save features and labels to disk\njoblib.dump((X, y), \"audio_features_labels.pkl\")\n\nprint(\"‚úÖ Features and labels saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:25:55.994059Z","iopub.execute_input":"2025-04-10T16:25:55.994863Z","iopub.status.idle":"2025-04-10T16:25:56.002456Z","shell.execute_reply.started":"2025-04-10T16:25:55.994834Z","shell.execute_reply":"2025-04-10T16:25:56.001236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load preprocessed features and labels\nX, y = joblib.load(\"audio_features_labels.pkl\")\n\nprint(\"‚úÖ Loaded saved data:\", X.shape, y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:26:02.789607Z","iopub.execute_input":"2025-04-10T16:26:02.79006Z","iopub.status.idle":"2025-04-10T16:26:02.797759Z","shell.execute_reply.started":"2025-04-10T16:26:02.79002Z","shell.execute_reply":"2025-04-10T16:26:02.796688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Encode the string labels to integers\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Split the dataset (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n\nprint(\"‚úÖ Training set:\", X_train.shape)\nprint(\"‚úÖ Testing set:\", X_test.shape)\nprint(\"‚úÖ Classes:\", le.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:26:06.598117Z","iopub.execute_input":"2025-04-10T16:26:06.598452Z","iopub.status.idle":"2025-04-10T16:26:06.633614Z","shell.execute_reply.started":"2025-04-10T16:26:06.598426Z","shell.execute_reply":"2025-04-10T16:26:06.632713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Train the model\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = clf.predict(X_test)\n\n# Evaluation\nprint(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred)*100)\nprint(\"üìä Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:26:44.630231Z","iopub.execute_input":"2025-04-10T16:26:44.630596Z","iopub.status.idle":"2025-04-10T16:26:45.73943Z","shell.execute_reply.started":"2025-04-10T16:26:44.630568Z","shell.execute_reply":"2025-04-10T16:26:45.738391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Save the trained model and the label encoder\njoblib.dump(clf, \"emotion_audio_rf_model.pkl\")\njoblib.dump(le, \"label_encoder.pkl\")\n\nprint(\"‚úÖ Model and Label Encoder saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:26:55.029754Z","iopub.execute_input":"2025-04-10T16:26:55.030137Z","iopub.status.idle":"2025-04-10T16:26:55.094987Z","shell.execute_reply.started":"2025-04-10T16:26:55.030107Z","shell.execute_reply":"2025-04-10T16:26:55.093907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Load saved model and label encoder\nmodel = joblib.load(\"emotion_audio_rf_model.pkl\")\nle = joblib.load(\"label_encoder.pkl\")\n\nprint(\"‚úÖ Model and Label Encoder loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:27:13.980277Z","iopub.execute_input":"2025-04-10T16:27:13.980636Z","iopub.status.idle":"2025-04-10T16:27:14.044016Z","shell.execute_reply.started":"2025-04-10T16:27:13.980599Z","shell.execute_reply":"2025-04-10T16:27:14.042841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport joblib\n\n# Load model and label encoder\nmodel = joblib.load(\"emotion_audio_rf_model.pkl\")\nle = joblib.load(\"label_encoder.pkl\")\n\n# Reusable feature extractor\ndef extract_features(file_path):\n    try:\n        y, sr = librosa.load(file_path, sr=None)\n\n        # Extract features\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n\n        # Combine feature vectors\n        features = np.hstack([\n            np.mean(mfcc, axis=1),\n            np.mean(chroma, axis=1),\n            np.mean(centroid, axis=1)\n        ])\n\n        return features\n    except Exception as e:\n        print(f\"‚ùå Error processing audio: {e}\")\n        return None\n\n# Path to your uploaded audio\nfile_path = \"/kaggle/input/recording/Recording.wav\"\nfeatures = extract_features(file_path)\n\nif features is not None:\n    features = features.reshape(1, -1)\n    pred = model.predict(features)\n    emotion = le.inverse_transform(pred)[0]\n    print(\"üéôÔ∏è Predicted Emotion:\", emotion)\nelse:\n    print(\"‚ö†Ô∏è Could not extract features.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:27:20.997731Z","iopub.execute_input":"2025-04-10T16:27:20.998134Z","iopub.status.idle":"2025-04-10T16:27:21.161551Z","shell.execute_reply.started":"2025-04-10T16:27:20.998102Z","shell.execute_reply":"2025-04-10T16:27:21.160303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Load model and label encoder\nmodel = joblib.load(\"emotion_audio_rf_model.pkl\")\nle = joblib.load(\"label_encoder.pkl\")\n\n# Load your audio\nfile_path = \"/kaggle/input/recording/Recording.wav\"\ny, sr = librosa.load(file_path, sr=None)\n\n# Extract features (same as training)\ndef extract_features(y, sr):\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n    return np.hstack([\n        np.mean(mfcc, axis=1),\n        np.mean(chroma, axis=1),\n        np.mean(centroid, axis=1)\n    ])\n\n# Predict\nfeatures = extract_features(y, sr).reshape(1, -1)\npred = model.predict(features)\nemotion = le.inverse_transform(pred)[0]\nprobs = model.predict_proba(features)[0]\n\n# Output\nprint(\"üéôÔ∏è Predicted Emotion:\", emotion)\nfor label, prob in zip(le.classes_, probs):\n    print(f\"{label}: {prob:.4f}\")\n\n# Plot\nplt.figure(figsize=(10, 4))\nplt.bar(le.classes_, probs, color='skyblue')\nplt.title(f\"Emotion Confidence Scores for {file_path.split('/')[-1]}\")\nplt.ylabel(\"Confidence\")\nplt.xlabel(\"Emotion\")\nplt.xticks(rotation=45)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:27:27.701471Z","iopub.execute_input":"2025-04-10T16:27:27.701848Z","iopub.status.idle":"2025-04-10T16:27:28.142954Z","shell.execute_reply.started":"2025-04-10T16:27:27.701819Z","shell.execute_reply":"2025-04-10T16:27:28.141764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa.display\n\n# Waveform\nplt.figure(figsize=(12, 3))\nlibrosa.display.waveshow(y, sr=sr)\nplt.title(\"üéµ Audio Waveform\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")\nplt.tight_layout()\nplt.show()\n\n# MFCC heatmap\n# Compute MFCCs with better resolution\nmfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=256)\n\nplt.figure(figsize=(12, 4))\nlibrosa.display.specshow(mfccs, x_axis='time', sr=sr, hop_length=256, cmap='magma')\nplt.colorbar(format='%+2.0f dB')\nplt.title('üéß MFCCs (Mel-Frequency Cepstral Coefficients) ‚Äî Enhanced')\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:27:34.220782Z","iopub.execute_input":"2025-04-10T16:27:34.221179Z","iopub.status.idle":"2025-04-10T16:27:34.998253Z","shell.execute_reply.started":"2025-04-10T16:27:34.221147Z","shell.execute_reply":"2025-04-10T16:27:34.996875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import whisper\nimport librosa\nimport numpy as np\nimport joblib\nimport json\nfrom datetime import datetime\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n# --- CONFIG ---\nfile_path = \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_angry/YAF_back_angry.wav\"\nuser_secrets = UserSecretsClient()  # <-- üîß Add this line\napi_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n# --- Load Models ---\nprint(\"üîÅ Loading models...\")\nemotion_model = joblib.load(\"emotion_audio_rf_model.pkl\")\nlabel_encoder = joblib.load(\"label_encoder.pkl\")\nwhisper_model = whisper.load_model(\"base\")\ngenai.configure(api_key=api_key)\ngemini_model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro\")\n\n# --- Transcription ---\nprint(\"üéôÔ∏è Transcribing audio...\")\nwhisper_result = whisper_model.transcribe(file_path)\ntranscript = whisper_result[\"text\"]\n\n# --- Feature Extraction ---\nprint(\"üìä Extracting audio features...\")\ny, sr = librosa.load(file_path, sr=None)\nmfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\nchroma = librosa.feature.chroma_stft(y=y, sr=sr)\ncentroid = librosa.feature.spectral_centroid(y=y, sr=sr)\nfeatures = np.hstack([\n    np.mean(mfcc, axis=1),\n    np.mean(chroma, axis=1),\n    np.mean(centroid, axis=1)\n]).reshape(1, -1)\n\n# --- Emotion Prediction ---\nprint(\"ü§ñ Predicting emotion...\")\nprobs = emotion_model.predict_proba(features)[0]\npred_idx = np.argmax(probs)\npred_emotion = label_encoder.inverse_transform([pred_idx])[0]\nmood_score = round(probs[pred_idx], 2)\nemotion_probs = dict(zip(label_encoder.classes_, map(lambda x: round(x, 2), probs)))\n\n# --- Gemini Prompt ---\nprint(\"üí¨ Asking Gemini for analysis...\")\ngemini_prompt = f\"\"\"\nYou are an empathetic AI therapist. Given the following voice transcript and predicted emotional state from an audio analysis, provide a gentle and supportive reflection.\n\nTranscript: \"{transcript}\"\n\nPredicted Emotion: {pred_emotion}\nEmotion Probabilities: {emotion_probs}\nMood Score: {mood_score}\n\nRespond in this structure:\nInsight: ...\nPossible Cause: ...\nSuggestion: ...\n\"\"\"\n\ngemini_response = gemini_model.generate_content(gemini_prompt)\ngemini_analysis = gemini_response.text.strip()\n\n# --- Session Summary ---\nsession_summary = {\n    \"timestamp\": datetime.now().isoformat(),\n    \"transcript\": transcript,\n    \"predicted_emotion\": pred_emotion,\n    \"mood_score\": mood_score,\n    \"full_probs\": emotion_probs,\n    \"gemini_analysis\": gemini_analysis\n}\n\n# --- Output ---\nprint(json.dumps(session_summary, indent=4))\n\nwith open(\"session_log.json\", \"w\") as f:\n    json.dump(session_summary, f, indent=4)\n\nprint(\"‚úÖ Session logged successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:20:32.742388Z","iopub.execute_input":"2025-04-10T17:20:32.742794Z","iopub.status.idle":"2025-04-10T17:20:41.284276Z","shell.execute_reply.started":"2025-04-10T17:20:32.742761Z","shell.execute_reply":"2025-04-10T17:20:41.283129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üå±AI Therapist ‚Äì Weekly Growth Session\n \nWe focus on **weekly group reflection and growth** using **RAG (Retrieval-Augmented Generation)** with **text input**.\n\n## üß† Core Features:\n- üìÑ **Text Input** (Group reflections, check-ins)\n- üîç **RAG Integration**: Retrieves relevant mental health insights or affirmations\n- üîÅ **Multi-turn Grounding**: Ongoing, context-aware conversation\n- ü§ó **Empathetic Gemini Responses**: Warm, human-like therapist replies\n\n---\n\n> \"Sometimes talking through your thoughts with a caring assistant can reveal new paths to healing.\"\n\nLet‚Äôs begin the session with group input... üåü\n","metadata":{}},{"cell_type":"markdown","source":"### Sample Group Input (Text Based)","metadata":{}},{"cell_type":"code","source":"group_inputs = [\n    \"I felt overwhelmed with work this week, like I‚Äôm losing balance.\",\n    \"Honestly, I‚Äôm trying hard to stay positive, but the stress is building.\",\n    \"I had one good day, but then felt like I relapsed emotionally.\",\n    \"I‚Äôm just tired, but I want to get better.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:32:46.112825Z","iopub.execute_input":"2025-04-10T16:32:46.113284Z","iopub.status.idle":"2025-04-10T16:32:46.117844Z","shell.execute_reply.started":"2025-04-10T16:32:46.113251Z","shell.execute_reply":"2025-04-10T16:32:46.116792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rag_context = \"\"\"\n[Supportive Tip] When feeling overwhelmed, it's helpful to break tasks into small, manageable steps and reward yourself for progress.\n\n[Affirmation] Your efforts are valid, even when results aren‚Äôt immediate. You‚Äôre showing strength by acknowledging how you feel.\n\n[Emotional Insight] Emotional relapses can happen during healing. What matters is recognizing them and continuing your journey.\n\n[Reminder] Rest is not a weakness. Recharging allows your mind and emotions to reset. Be gentle with yourself.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:33:00.014868Z","iopub.execute_input":"2025-04-10T16:33:00.015312Z","iopub.status.idle":"2025-04-10T16:33:00.019628Z","shell.execute_reply.started":"2025-04-10T16:33:00.015281Z","shell.execute_reply":"2025-04-10T16:33:00.018536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Passing To Gemini Model","metadata":{}},{"cell_type":"code","source":"group_input = \"\"\"\nHey, I've been feeling super drained this week. Nothing feels exciting anymore.\nEven when I try to relax, my mind keeps racing about all the things I'm not doing right.\n\"\"\"\n\ngemini_prompt = f\"\"\"\nYou are an empathetic AI therapist. This is a group therapy check-in session.\n\nEach participant shares their emotional state. Below is one input, along with some supportive context from trusted therapeutic sources (RAG).\n\nGroup Member Input:\n\\\"\\\"\\\"{group_input}\\\"\\\"\\\"\n\nContext from Therapist Resources:\n\\\"\\\"\\\"{rag_context}\\\"\\\"\\\"\n\nYour task is to respond with:\n1. A reflection that shows deep emotional understanding.\n2. A grounding suggestion personalized to this input.\n3. A gentle question for the participant to reflect on further.\n\nKeep your tone supportive and warm.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:33:44.115199Z","iopub.execute_input":"2025-04-10T16:33:44.115573Z","iopub.status.idle":"2025-04-10T16:33:44.120277Z","shell.execute_reply.started":"2025-04-10T16:33:44.115543Z","shell.execute_reply":"2025-04-10T16:33:44.11912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemini_response = gemini_model.generate_content(gemini_prompt)\nresponse_text = gemini_response.text.strip()\nprint(response_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:34:06.630555Z","iopub.execute_input":"2025-04-10T16:34:06.630951Z","iopub.status.idle":"2025-04-10T16:34:12.078399Z","shell.execute_reply.started":"2025-04-10T16:34:06.630918Z","shell.execute_reply":"2025-04-10T16:34:12.077099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Multi Turn Capability","metadata":{}},{"cell_type":"code","source":"# --- Simulate User Reply ---\nuser_reply = \"I tried what you said. I cleaned my desk and honestly, it helped a bit. But the feeling still lingers. I don‚Äôt know how to stop overthinking.\"\n\n# --- Gemini Follow-up Prompt ---\nmulti_turn_prompt = f\"\"\"\nContinue this therapy session as an empathetic AI therapist.\n\nPrevious Reflection: {response_text}\n\nUser Follow-up: \"{user_reply}\"\n\nNow, continue the session in the same style:\n1. Reflection: ...\n2. Grounding Suggestion: ...\n3. Reflection Prompt: ...\n\"\"\"\n\n# --- Gemini Multi-turn Response ---\nmulti_turn_response = gemini_model.generate_content(multi_turn_prompt)\nfollowup_text = multi_turn_response.text.strip()\nprint(followup_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:35:03.91293Z","iopub.execute_input":"2025-04-10T16:35:03.913383Z","iopub.status.idle":"2025-04-10T16:35:09.655355Z","shell.execute_reply.started":"2025-04-10T16:35:03.913351Z","shell.execute_reply":"2025-04-10T16:35:09.654225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weekly_growth_log = {\n    \"timestamp\": datetime.now().isoformat(),\n    \"predicted_emotion\": pred_emotion,\n    \"mood_score\": mood_score,\n    \"emotion_probs\": emotion_probs,\n    \"transcript\": transcript,\n    \"gemini_initial_response\": response_text,\n    \"user_follow_up\": user_reply,\n    \"gemini_follow_up_response\": followup_text\n}\n\n# Save full session\nwith open(\"session_growth_log.json\", \"w\") as f:\n    json.dump(weekly_growth_log, f, indent=4)\n\nprint(\"üß† Weekly Growth Session logged successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:35:47.369523Z","iopub.execute_input":"2025-04-10T16:35:47.369935Z","iopub.status.idle":"2025-04-10T16:35:47.378004Z","shell.execute_reply.started":"2025-04-10T16:35:47.369899Z","shell.execute_reply":"2025-04-10T16:35:47.376379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõë Crisis Alert ‚Äì Suicide Prevention Prompt\n\n---\n\n### üìò Module Description  \nThis module acts as an emergency detection system using voice input. It aims to identify critical emotional states such as **despair**, **fear**, or **overwhelming sadness** using trained emotion detection models.  \nIf a voice input exceeds a crisis emotion threshold, an **immediate alert response** is triggered.\n\n---\n\n### üéØ Objectives  \n- Detect emergency emotional signals from voice  \n- Trigger crisis response workflow  \n- Log incident details for support or escalation  \n\n---\n\n### üé§ Audio Input (Emergency)  \nUsers upload or record a voice clip during an emotional moment.  \nThe system analyzes emotional tones and flags potential **crisis patterns**.\n\n---\n\n### üß† Voice Emotion Classification  \nUses pre-trained emotion model to compute probabilities for the following emotional states:\n- üò¢ **Sadness**  \n- üò® **Fear**  \n- üíî **Despair**  \n- üò† **Anger**  \n- üòê **Neutral**\n\n> If any **crisis-prone emotion exceeds 70%**, the system **automatically triggers an emergency alert**.\n\n","metadata":{}},{"cell_type":"markdown","source":"###  Crisis Alert ‚Äì Suicide Prevention Prompt","metadata":{}},{"cell_type":"code","source":"import joblib\nimport librosa\nimport numpy as np\nfrom datetime import datetime\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:06.412843Z","iopub.execute_input":"2025-04-10T17:44:06.413242Z","iopub.status.idle":"2025-04-10T17:44:06.418108Z","shell.execute_reply.started":"2025-04-10T17:44:06.413209Z","shell.execute_reply":"2025-04-10T17:44:06.416804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Load Models ---\nemotion_model = joblib.load(\"/kaggle/working/emotion_audio_rf_model.pkl\")\nlabel_encoder = joblib.load(\"/kaggle/working/label_encoder.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:07.682941Z","iopub.execute_input":"2025-04-10T17:44:07.683323Z","iopub.status.idle":"2025-04-10T17:44:07.733207Z","shell.execute_reply.started":"2025-04-10T17:44:07.683294Z","shell.execute_reply":"2025-04-10T17:44:07.732143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Predict Emotion ---\nprobs = emotion_model.predict_proba(features)[0]\nclasses = label_encoder.classes_\nemotion_probs = dict(zip(classes, map(lambda x: round(x, 2), probs)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:08.750778Z","iopub.execute_input":"2025-04-10T17:44:08.751169Z","iopub.status.idle":"2025-04-10T17:44:08.766749Z","shell.execute_reply.started":"2025-04-10T17:44:08.75114Z","shell.execute_reply":"2025-04-10T17:44:08.765722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_emotion_crisis(file_path, emotion_model, label_encoder, CRISIS_EMOTIONS, CRISIS_THRESHOLD):\n    \"\"\"\n    Analyze audio file for emotional content and detect potential crisis situations.\n    \n    Args:\n        file_path: Path to the audio file\n        emotion_model: Trained model for emotion detection\n        label_encoder: Label encoder for emotion classes\n        CRISIS_EMOTIONS: List of emotions that indicate a crisis\n        CRISIS_THRESHOLD: Probability threshold for crisis detection\n        \n    Returns:\n        dict: Results of the analysis including crisis status and emotion probabilities\n    \"\"\"\n    import librosa\n    import numpy as np\n    import json\n    from datetime import datetime\n    \n    # Extract Audio Features\n    y, sr = librosa.load(file_path, sr=None)\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n    features = np.hstack([\n        np.mean(mfcc, axis=1),\n        np.mean(chroma, axis=1),\n        np.mean(centroid, axis=1)\n    ]).reshape(1, -1)\n    \n    # Predict Emotion\n    probs = emotion_model.predict_proba(features)[0]\n    classes = label_encoder.classes_\n    emotion_probs = dict(zip(classes, map(lambda x: round(x, 2), probs)))\n    \n    # Crisis Detection\n    triggered = False\n    triggered_emotion = None\n    triggered_score = None\n    \n    for emotion, prob in emotion_probs.items():\n        if emotion.lower() in CRISIS_EMOTIONS and prob > CRISIS_THRESHOLD:\n            triggered = True\n            triggered_emotion = emotion\n            triggered_score = prob\n            break\n    \n    # Log or Respond\n    result = {}\n    \n    if triggered:\n        crisis_log = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"crisis_emotion\": triggered_emotion,\n            \"probability\": round(triggered_score, 2),\n            \"status\": \"‚ö†Ô∏è Crisis Alert Triggered\"\n        }\n        with open(\"crisis_log.json\", \"w\") as f:\n            json.dump(crisis_log, f, indent=4)\n        print(\"üö® CRISIS ALERT TRIGGERED\")\n        print(json.dumps(crisis_log, indent=4))\n        result = crisis_log\n    else:\n        print(\"‚úÖ No Crisis Detected\")\n        print(\"Emotion Probabilities:\", emotion_probs)\n        result = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"status\": \"No Crisis Detected\",\n            \"emotion_probabilities\": emotion_probs\n        }\n    \n    return result\n\n# Example usage\n# result = detect_emotion_crisis(\n#     file_path=\"path/to/audio.wav\", \n#     emotion_model=emotion_model,\n#     label_encoder=label_encoder,\n#     CRISIS_EMOTIONS=[\"anger\", \"fear\", \"distress\"],\n#     CRISIS_THRESHOLD=0.7\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:10.325201Z","iopub.execute_input":"2025-04-10T17:44:10.325573Z","iopub.status.idle":"2025-04-10T17:44:10.335726Z","shell.execute_reply.started":"2025-04-10T17:44:10.325542Z","shell.execute_reply":"2025-04-10T17:44:10.334522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = detect_emotion_crisis(\n    file_path=\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_angry/YAF_back_angry.wav\", \n    emotion_model=emotion_model,\n    label_encoder=label_encoder,\n    CRISIS_EMOTIONS = [\"angry\", \"fearful\", \"distress\"],\n    CRISIS_THRESHOLD=0.3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:13.304687Z","iopub.execute_input":"2025-04-10T17:44:13.30509Z","iopub.status.idle":"2025-04-10T17:44:13.347937Z","shell.execute_reply.started":"2025-04-10T17:44:13.305059Z","shell.execute_reply":"2025-04-10T17:44:13.347075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = detect_emotion_crisis(\n    file_path=\"/kaggle/input/recording/Recording.wav\", \n    emotion_model=emotion_model,\n    label_encoder=label_encoder,\n    CRISIS_EMOTIONS = [\"angry\", \"fearful\", \"distress\"],\n    CRISIS_THRESHOLD=0.3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:14.476291Z","iopub.execute_input":"2025-04-10T17:44:14.476623Z","iopub.status.idle":"2025-04-10T17:44:14.544936Z","shell.execute_reply.started":"2025-04-10T17:44:14.476598Z","shell.execute_reply":"2025-04-10T17:44:14.544008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß† Conclusion\n\nThe **AI Therapist: A GenAI-Powered Mental Health Assistant** demonstrates the powerful potential of combining emotional understanding with state-of-the-art generative models like **Gemini 1.5 Pro**. This solution blends real-time audio and text mood detection with empathetic feedback, making mental health support more accessible and personalized.\n\n**Key Highlights:**\n- üéôÔ∏è Emotion detection from voice using ML\n- üìù Text-based mood tracking with grounding\n- üí¨ Gemini-powered reflections and suggestions\n- üö® Crisis Alert Module for emergency detection\n- üå± Weekly Growth Sessions for long-term well-being\n\nBy integrating these features, this assistant provides continuous, compassionate support‚Äîbridging technology with care.\n\n---\n\n## üì¨ Contact Me\n\nFeel free to reach out or connect with me:\n\n- üíª **GitHub**: [github.com/ANJAN672](https://github.com/ANJAN672)  \n- üíº **LinkedIn**: [linkedin.com/in/anjan-b-35a884295](https://www.linkedin.com/in/anjan-b-35a884295/)  \n- üìß **Email**: anjan.b.s.007@gmail.com\n\n---\n\nThank you for exploring this project! üåü\n","metadata":{}}]}